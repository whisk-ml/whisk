{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Whisk\n",
    "Whisk makes it easy to create reproducible, collaborative machine learning projects. It provides the project guide rails so you can focus on the data science. Here's what you need to know get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "This project comes pre-loaded with a virtual environment named `venv` and an IPython kernel named `{{cookiecutter.project_name}}`. This notebooks uses the `{{cookiecutter.project_name}}` kernel. In the terminal, run `source venv/bin/activate` to activate the venv.\n",
    "\n",
    "Dependencies are listed in the `requirements.txt` file. Add your dependencies to this file and run `pip install -r requirements.txt` to update your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading code from Python files\n",
    "When your notebook goes beyond exploratory work, it's a good practice to move your functions and classes to Python files. These are easier to maintain than notebook cells.\n",
    "\n",
    "The cell below ensures that your notebook cells always have a fresh copy of the `src` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"autoreload\" extension. Prior to executing code, modules are reloaded. \n",
    "# There's no need to restart jupyter notebook if you modify code in the `src` directory.\n",
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, `src/{{cookiecutter.project_name}}/data/extract.py` contains a sample function named `extract_example()`. You can call this function from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{cookiecutter.project_name}}.data.extract import *\n",
    "extract_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data directory\n",
    "Training data should be in version control alongside your code to ensure experiments are reproducible. For smaller training tests, it is OK to store in Git. For larger training sets, DVC is pre-installed.\n",
    "\n",
    "Place your data inside the project's data directory. You can obtain the path to this directory like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {{cookiecutter.project_name}}\n",
    "\n",
    "{{cookiecutter.project_name}}.data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving models to the artifacts directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training a model you should save it to disk so you can invoke the model later. The method call for saving a model to disk is dependent on your ML framework (for example, Scikit-learn uses pickle while you just call `save` on a PyTorch model).\n",
    "\n",
    "Regardless of your ML framework, save your model and required artifacts for pre/post-processing to the artifacts directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {{cookiecutter.project_name}}\n",
    "\n",
    "{{cookiecutter.project_name}}.artifacts_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a model looks this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {{cookiecutter.project_name}}\n",
    "from whisk.model_stub import ModelStub # A fake model\n",
    "# This example uses pickle to serialize a Python object. \n",
    "# Use the preferred serialization approach for your ML framework.\n",
    "import pickle\n",
    "\n",
    "model = ModelStub()\n",
    "file_path = {{cookiecutter.project_name}}.artifacts_dir / \"model.pkl\"\n",
    "pickle.dump(model, open(file_path,\"wb\"))\n",
    "print(f\"Saved model stub to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking a saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project includes a sample `data.models.Model` class that loads a model from disk and allows you to generate a prediction. Find this class inside the `src/{{cookiecutter.project_name}}/models/model.py` file. You can invoke the model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{cookiecutter.project_name}}.models.model import Model\n",
    "model = Model()\n",
    "model.predict([[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update `src/{{cookiecutter.project_name}}/models/model.py` to handle loading and pre/post-processing for your own model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "{{cookiecutter.project_name}}",
   "language": "python",
   "name": "{{cookiecutter.project_name}}"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
